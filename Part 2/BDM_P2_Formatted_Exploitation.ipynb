{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woshimajintao/BDM-Project/blob/main/Part%202/BDM_P2_Formatted_Exploitation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtl59G6OS8Ml",
        "outputId": "acd4ac5c-e601-498c-9420-59ff6e4351c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: delta-spark in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (3.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (7.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark) (3.19.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark<3.6.0,>=3.5.0->delta-spark) (0.10.9.7)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.2)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.25.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.1)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.43)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.3)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.6)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.25.2)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.25.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.10/dist-packages (from mlflow) (24.1)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.3)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.20.3)\n",
            "Requirement already satisfied: pyarrow<16,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (14.0.2)\n",
            "Requirement already satisfied: pytz<2025 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2023.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.0.1)\n",
            "Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.4)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.11.4)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.30)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.5.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\n",
            "Requirement already satisfied: gunicorn<23 in /usr/local/lib/python3.10/dist-packages (from mlflow) (22.0.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.0.7)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: aniso8601<10,>=8 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (9.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow) (1.2.14)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow) (0.46b0)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (2024.6.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow) (1.14.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install delta-spark\n",
        "!pip install statsmodels\n",
        "!pip install mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "VSlth008TJDF"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.spark\n",
        "from google.colab import drive\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
        "from pyspark.mllib.evaluation import RegressionMetrics\n",
        "from pyspark.mllib.feature import StandardScaler\n",
        "from pyspark.sql.functions import col, avg, count, when, datediff, current_date, to_date, regexp_replace, month, dayofmonth, sum as _sum, coalesce, lit, isnan, unix_timestamp, lag, from_json, lead\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, LongType, StringType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from delta.tables import DeltaTable\n",
        "from delta import configure_spark_with_delta_pip\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import glob\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7K0Q66qdxrv",
        "outputId": "9894368f-6004-49e2-f877-59f13f2664a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCAKDf199cFO",
        "outputId": "91dfb85d-2b9d-49e1-abc5-e698eec4cace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delta Lake loaded: True\n"
          ]
        }
      ],
      "source": [
        "# Initialize a Spark session\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"DataProcessingWithDeltaLake\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "delta_extensions = spark.conf.get(\"spark.sql.extensions\", \"\")\n",
        "print(\"Delta Lake loaded:\", \"io.delta.sql.DeltaSparkSessionExtension\" in delta_extensions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-XMN9uI7sRa"
      },
      "source": [
        "# Formatter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI8garVwi0Qh"
      },
      "source": [
        "## Process Airport Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdU95cMdi5Rn"
      },
      "source": [
        "Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzroU55WjDe_",
        "outputId": "3501296d-5761-4964-bf6f-892da4e518e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------+--------------------+-----------+\n",
            "|Data_Referencia|Codi_Pais|            Nom_Pais|Nombre_Vols|\n",
            "+---------------+---------+--------------------+-----------+\n",
            "|     2023-01-01|        1|             Espanya|        193|\n",
            "|     2023-01-01|        2|              Itàlia|         63|\n",
            "|     2023-01-01|        3|          Regne Unit|         57|\n",
            "|     2023-01-01|        4|              França|         63|\n",
            "|     2023-01-01|        5|            Alemanya|         50|\n",
            "|     2023-01-01|        6|            Portugal|         26|\n",
            "|     2023-01-01|        7|       Països Baixos|         28|\n",
            "|     2023-01-01|        8|              Suïssa|         26|\n",
            "|     2023-01-01|        9|             Bèlgica|         10|\n",
            "|     2023-01-01|       10|          Marroc, el|         21|\n",
            "|     2023-01-01|       11|   Estats Units, els|          9|\n",
            "|     2023-01-01|       12|             Àustria|          5|\n",
            "|     2023-01-01|       13|             Irlanda|         12|\n",
            "|     2023-01-01|       14|             Turquia|         10|\n",
            "|     2023-01-01|       15|             Polònia|          8|\n",
            "|     2023-01-01|       16|           Dinamarca|         10|\n",
            "|     2023-01-01|       17|             Romania|          6|\n",
            "|     2023-01-01|       18|              Grècia|          4|\n",
            "|     2023-01-01|       19|              Suècia|         10|\n",
            "|     2023-01-01|       20|Emirats Àrabs Uni...|          6|\n",
            "+---------------+---------+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/drive/MyDrive/BDM/Data/Landing_Zone/2023_TransitAeri_FlightRadar_Ppal_Pais.csv\"\n",
        "airport_df = spark.read.options(inferSchema=True, header=True).csv(file_path)\n",
        "airport_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4sWCnqYi5cB"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVgsQJgAjEG0",
        "outputId": "42eaeb42-eeb0-440c-bfe4-fe08d619dff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------+--------------------+-----------+\n",
            "|Data_Referencia|Codi_Pais|            Nom_Pais|Nombre_Vols|\n",
            "+---------------+---------+--------------------+-----------+\n",
            "|     2023-01-02|        6|            Portugal|         29|\n",
            "|     2023-01-30|        7|       Països Baixos|         20|\n",
            "|     2023-02-21|        3|          Regne Unit|         58|\n",
            "|     2023-02-22|       13|             Irlanda|          8|\n",
            "|     2023-03-03|       23|               Qatar|          4|\n",
            "|     2023-03-09|        2|              Itàlia|         75|\n",
            "|     2023-03-16|        2|              Itàlia|         74|\n",
            "|     2023-03-16|       23|               Qatar|          4|\n",
            "|     2023-03-17|        7|       Països Baixos|         28|\n",
            "|     2023-03-28|        5|            Alemanya|         65|\n",
            "|     2023-03-31|        6|            Portugal|         40|\n",
            "|     2023-04-02|       20|Emirats Àrabs Uni...|          6|\n",
            "|     2023-04-03|        9|             Bèlgica|         22|\n",
            "|     2023-04-18|       16|           Dinamarca|         10|\n",
            "|     2023-04-23|        9|             Bèlgica|         16|\n",
            "|     2023-04-30|       26|             Noruega|          8|\n",
            "|     2023-05-10|       22|             Hongria|          4|\n",
            "|     2023-05-14|        2|              Itàlia|         96|\n",
            "|     2023-06-19|        9|             Bèlgica|         19|\n",
            "|     2023-06-28|       22|             Hongria|          4|\n",
            "+---------------+---------+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- Data_Referencia: date (nullable = true)\n",
            " |-- Codi_Pais: integer (nullable = true)\n",
            " |-- Nom_Pais: string (nullable = true)\n",
            " |-- Nombre_Vols: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Drop duplicates\n",
        "airport_df_cleaned = airport_df.dropDuplicates()\n",
        "\n",
        "# Handle missing values\n",
        "airport_df_cleaned = airport_df_cleaned.dropna()\n",
        "\n",
        "airport_df_cleaned.show()\n",
        "airport_df_cleaned.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8ZRsBdMi5lK"
      },
      "source": [
        "Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr7Mz7ZXio6i",
        "outputId": "c6d51903-eb80-4317-ad7e-8a08d804e9c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|      Date|Germany_Tourists|Spain_Tourists|France_Tourists|Italy_Tourists|UK_Tourists|Total_Tourists|\n",
            "+----------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|2023-01-01|              50|           193|             63|            63|         57|           426|\n",
            "|2023-01-02|              56|           212|             67|            82|         73|           490|\n",
            "|2023-01-03|              54|           227|             57|            80|         59|           477|\n",
            "|2023-01-04|              57|           217|             54|            71|         62|           461|\n",
            "|2023-01-05|              53|           195|             55|            79|         55|           437|\n",
            "+----------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate total tourists per country and find top 5 countries\n",
        "country_tourists = airport_df_cleaned.groupBy(\"Nom_Pais\").agg(_sum(\"Nombre_Vols\").alias(\"total_tourists\"))\n",
        "top_countries = country_tourists.orderBy(col(\"total_tourists\").desc()).limit(5)\n",
        "top_country_list = [row['Nom_Pais'] for row in top_countries.collect()]\n",
        "\n",
        "# Filter and pivot the DataFrame for the top 5 countries\n",
        "pivot_df = airport_df_cleaned.filter(col(\"Nom_Pais\").isin(top_country_list)).groupBy(\"Data_Referencia\").pivot(\"Nom_Pais\").agg(_sum(\"Nombre_Vols\"))\n",
        "\n",
        "# Add a new column for total tourists and fill NaNs with zeros\n",
        "for country in top_country_list:\n",
        "    pivot_df = pivot_df.withColumn(country, coalesce(col(country), lit(0)))\n",
        "\n",
        "pivot_df = pivot_df.withColumn(\"total_tourists\", sum(col(country) for country in top_country_list))\n",
        "\n",
        "airport_df_final = pivot_df.orderBy(\"Data_Referencia\")\n",
        "\n",
        "column_renames = {\n",
        "    \"Data_Referencia\": \"Date\",\n",
        "    \"Alemanya\": \"Germany_Tourists\",\n",
        "    \"Espanya\": \"Spain_Tourists\",\n",
        "    \"França\": \"France_Tourists\",\n",
        "    \"Itàlia\": \"Italy_Tourists\",\n",
        "    \"Regne Unit\": \"UK_Tourists\",\n",
        "    \"total_tourists\": \"Total_Tourists\"\n",
        "}\n",
        "\n",
        "for old_col, new_col in column_renames.items():\n",
        "    airport_df_final = airport_df_final.withColumnRenamed(old_col, new_col)\n",
        "\n",
        "airport_df_final.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZvvhofHjGsP"
      },
      "source": [
        "## Process Listing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDhJvSPfjGsQ"
      },
      "source": [
        "Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR95nXHtjGsQ",
        "outputId": "65c6477c-a8a6-4aa3-ba87-b2a5c1f08662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+-------+--------------+-------------------+--------------------+-----------------+-----------------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+\n",
            "|    id|                name|host_id|     host_name|neighbourhood_group|       neighbourhood|         latitude|        longitude|      room_type|price|minimum_nights|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|    license|\n",
            "+------+--------------------+-------+--------------+-------------------+--------------------+-----------------+-----------------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+\n",
            "| 17475|Rental unit in 08...|  65623|          Luca|           Eixample|la Dreta de l'Eix...|         41.39939|          2.17044|Entire home/apt|  140|             5|               26| 2023-12-04|             0.16|                             1|              32|                    9|       NULL|\n",
            "| 18674|Rental unit in Ba...|  71615| Mireia  Maria|           Eixample|  la Sagrada Família|         41.40556|          2.17262|Entire home/apt|  121|             1|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|\n",
            "|198958|Rental unit in Ba...| 971768|         Laura|         Sant Martí|Diagonal Mar i el...|         41.40805|          2.21487|Entire home/apt|  304|             2|              105| 2023-10-16|             0.74|                             9|             137|                   26|HUTB-000926|\n",
            "| 23197|Rental unit in Sa...|  90417|Etain (Marnie)|         Sant Martí|el Besòs i el Mar...|41.41243172529066|2.219750335269476|Entire home/apt|  200|             3|               75| 2023-11-25|             0.48|                             2|             300|                   11| HUTB005057|\n",
            "| 32711|Rental unit in Ba...| 135703|          Nick|             Gràcia|el Camp d'en Gras...|         41.40566|          2.17015|Entire home/apt|   79|             1|               99| 2023-10-18|             0.66|                             3|             297|                   16|HUTB-001722|\n",
            "+------+--------------------+-------+--------------+-------------------+--------------------+-----------------+-----------------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/drive/MyDrive/BDM/Data/Landing_Zone/listings.csv\"\n",
        "listings_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "listings_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRfbRiLwjGsQ"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vvzY9dz-jGsQ"
      },
      "outputs": [],
      "source": [
        "listings_df_cleaned = listings_df.dropDuplicates().dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdcpmTP2jGsQ"
      },
      "source": [
        "Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK4UxuYojGsQ",
        "outputId": "80fe49b8-6826-4139-871d-71a35d406ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+-------+----------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-------------+----------------------+-----------------+\n",
            "|    id|                name|host_id| host_name|neighbourhood_group|       neighbourhood|latitude|longitude|      room_type|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|      license|days_since_last_review|room_type_encoded|\n",
            "+------+--------------------+-------+----------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-------------+----------------------+-----------------+\n",
            "|206167|Rental unit in Ba...|1014050|   Minerva|     Sants-Montjuïc|        el Poble Sec| 41.3724|   2.1648|Entire home/apt|              142| 2023-10-12|             0.96|                             1|               0|                   22|  HUTB-009804|                   251|                1|\n",
            "|576046|Rental unit in Ba...|2833463|    Josefa|           Eixample|  la Sagrada Família|41.40315|  2.18462|   Private room|               52| 2023-07-31|             0.58|                             2|             314|                    1|       Exempt|                   324|                2|\n",
            "|745275|Rental unit in Ba...|3678711|         J|             Gràcia|Vallcarca i els P...|41.41073|  2.14704|Entire home/apt|               46| 2023-11-19|             0.36|                            10|             168|                    5|HUTB - 004040|                   213|                1|\n",
            "|811918|Rental unit in Ho...|4269183|      Alba|          Les Corts|la Maternitat i S...| 41.3779|  2.11991|Entire home/apt|              456| 2023-11-29|             3.42|                             5|              93|                   37|  HUTB-007730|                   203|                1|\n",
            "|715036|Rental unit in Ba...|3682696|Montserrat|     Sants-Montjuïc|        el Poble Sec|41.37455|  2.16669|Entire home/apt|              329| 2023-11-29|             2.49|                             1|             177|                   48|  HUTB-007660|                   203|                1|\n",
            "+------+--------------------+-------+----------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-------------+----------------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "listings_df_cleaned = listings_df_cleaned.fillna({'license': 'Unknown', 'reviews_per_month': 0})\n",
        "\n",
        "# Convert price to integer\n",
        "listings_df_cleaned = listings_df_cleaned.withColumn(\"price\", col(\"price\").cast(\"integer\"))\n",
        "\n",
        "# Calculate average price per neighbourhood\n",
        "avg_price_per_neighbourhood = listings_df_cleaned.groupBy(\"neighbourhood\").agg(avg(\"price\").alias(\"avg_price\"))\n",
        "\n",
        "# Calculate number of listings per neighbourhood\n",
        "count_listings_per_neighbourhood = listings_df_cleaned.groupBy(\"neighbourhood\").agg(count(\"id\").alias(\"listings_count\"))\n",
        "\n",
        "# Handle date columns\n",
        "listings_df_cleaned = listings_df_cleaned.withColumn(\"last_review\", col(\"last_review\").cast(\"date\"))\n",
        "listings_df_cleaned = listings_df_cleaned.withColumn(\"days_since_last_review\", datediff(current_date(), col(\"last_review\")))\n",
        "\n",
        "listings_df_final = listings_df_cleaned.withColumn(\n",
        "    \"room_type_encoded\",\n",
        "    when(col(\"room_type\") == \"Entire home/apt\", 1)\n",
        "    .when(col(\"room_type\") == \"Private room\", 2)\n",
        "    .when(col(\"room_type\") == \"Shared room\", 3)\n",
        "    .when(col(\"room_type\") == \"Hotel room\", 4)\n",
        "    .otherwise(0)\n",
        ")\n",
        "\n",
        "listings_df_final = listings_df_final.drop(\"price\")\n",
        "listings_df_final = listings_df_final.drop(\"minimum_nights\")\n",
        "listings_df_final = listings_df_final.drop(\"maximum_nights\")\n",
        "\n",
        "listings_df_final.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrM_dR6xjMme"
      },
      "source": [
        "## Process Calendar Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEUjOco2jMme"
      },
      "source": [
        "Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U8E0DT0jMme",
        "outputId": "dc890cee-9a62-4d5f-fa66-3658d04fdeb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+---------+-------+--------------+--------------+--------------+\n",
            "|listing_id|      date|available|  price|adjusted_price|minimum_nights|maximum_nights|\n",
            "+----------+----------+---------+-------+--------------+--------------+--------------+\n",
            "|    198958|2023-12-14|        t|$190.00|          NULL|             2|           365|\n",
            "|    198958|2023-12-15|        t|$190.00|          NULL|             2|           365|\n",
            "|    198958|2023-12-16|        t|$190.00|          NULL|             2|           365|\n",
            "|    198958|2023-12-17|        t|$190.00|          NULL|             2|           365|\n",
            "|    198958|2023-12-18|        t|$190.00|          NULL|             2|           365|\n",
            "+----------+----------+---------+-------+--------------+--------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/drive/MyDrive/BDM/Data/Landing_Zone/calendar.csv\"\n",
        "calendar_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "calendar_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yqyhiWTjMmf"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7ymrFWSOjMmf"
      },
      "outputs": [],
      "source": [
        "calendar_df_cleaned = calendar_df.dropDuplicates()\n",
        "calendar_df_cleaned.cache()\n",
        "\n",
        "# Remove currency symbols and commas from the price column, then convert to float\n",
        "calendar_df_cleaned = calendar_df_cleaned.withColumn(\"price\", regexp_replace(col(\"price\"), \"[$,]\", \"\").cast(\"float\"))\n",
        "calendar_df_cleaned = calendar_df_cleaned.dropna(subset=[\"price\"])\n",
        "\n",
        "avg_price = calendar_df_cleaned.agg(avg(\"price\")).first()[0]\n",
        "\n",
        "# Handle missing values by filling missing prices with the average price\n",
        "calendar_df_cleaned = calendar_df_cleaned.na.fill({'price': avg_price})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jRDBsFdjMmf"
      },
      "source": [
        "Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stBr9Q_WjMmf",
        "outputId": "2d2558ab-99cf-4ad8-d9e2-e8c49218f423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+---------+------+--------------+--------------+--------------+-----------------------+\n",
            "|listing_id|      date|available| price|adjusted_price|minimum_nights|maximum_nights|availability_percentage|\n",
            "+----------+----------+---------+------+--------------+--------------+--------------+-----------------------+\n",
            "|     36763|2024-05-12|        t|  30.0|          NULL|            31|            65|     57.534246575342465|\n",
            "|     36763|2024-02-07|        f|  30.0|          NULL|            31|            65|     57.534246575342465|\n",
            "|     40983|2024-11-20|        t| 205.0|          NULL|             2|           364|      78.63013698630137|\n",
            "|     46153|2024-03-03|        f|  70.0|          NULL|            31|          1125|      66.57534246575342|\n",
            "|  20472746|2024-11-28|        t|1292.0|     $1,292.00|             4|             7|      88.76712328767124|\n",
            "+----------+----------+---------+------+--------------+--------------+--------------+-----------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "calendar_df_cleaned = calendar_df_cleaned.withColumn(\"date\", to_date(col(\"date\"), 'yyyy-MM-dd'))\n",
        "\n",
        "# Calculate availability percentage per listing_id\n",
        "availability_df = calendar_df_cleaned.groupBy(\"listing_id\").agg(\n",
        "    (count(when(col(\"available\") == \"t\", 1)) / count(\"available\") * 100).alias(\"availability_percentage\")\n",
        ")\n",
        "\n",
        "# Join availability_df back to the main dataframe\n",
        "calendar_df_final = calendar_df_cleaned.join(availability_df, on=\"listing_id\", how=\"left\")\n",
        "\n",
        "# Show the transformed DataFrame\n",
        "calendar_df_final.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljN-FFzCtgSf"
      },
      "source": [
        "## Data Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i85umAL9tiap",
        "outputId": "6b7d6a52-c2b1-4193-e622-7ed4d58a1acb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+--------------+-----------------------+\n",
            "|   id|                name|host_id|host_name|neighbourhood_group|       neighbourhood|latitude|longitude|      room_type|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|    license|days_since_last_review|room_type_encoded|      date|available|price|adjusted_price|minimum_nights|maximum_nights|availability_percentage|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+--------------+-----------------------+\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-10-29|        t|205.0|          NULL|             2|           364|      78.63013698630137|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-09-11|        t|205.0|          NULL|             2|           364|      78.63013698630137|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-07-24|        t|205.0|          NULL|             2|           364|      78.63013698630137|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-03-21|        t|205.0|          NULL|             2|           364|      78.63013698630137|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-11-01|        t|205.0|          NULL|             2|           364|      78.63013698630137|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+--------------+-----------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Merge Listings and Calendar\n",
        "merged_df = listings_df_final.join(calendar_df_final, listings_df_final.id == calendar_df_final.listing_id, \"inner\")\n",
        "merged_df = merged_df.drop(calendar_df_final.listing_id)\n",
        "merged_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iInsXp-evYnk",
        "outputId": "dde64dbd-ed68-473b-8d06-3c108d96d13b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|   id|                name|host_id|host_name|neighbourhood_group|       neighbourhood|latitude|longitude|      room_type|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|    license|days_since_last_review|room_type_encoded|      date|available|price|minimum_nights|maximum_nights|availability_percentage|Germany_Tourists|Spain_Tourists|France_Tourists|Italy_Tourists|UK_Tourists|Total_Tourists|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-10-29|        t|205.0|             2|           364|      78.63013698630137|              68|           238|             87|            93|         94|           580|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-09-11|        t|205.0|             2|           364|      78.63013698630137|              73|           275|             80|           105|         89|           622|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-07-24|        t|205.0|             2|           364|      78.63013698630137|              78|           283|             78|           102|         83|           624|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-03-21|        t|205.0|             2|           364|      78.63013698630137|              61|           247|             57|            73|         65|           503|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-11-01|        t|205.0|             2|           364|      78.63013698630137|              66|           236|             66|            98|         66|           532|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Merge with Airport Data\n",
        "merged_df = merged_df.withColumn(\"month_day\", col(\"date\").substr(6, 5))\n",
        "airport_df_final = airport_df_final.withColumn(\"month_day\", col(\"Date\").substr(6, 5))\n",
        "final_merged_df = merged_df.join(airport_df_final.drop(\"Date\"), on=\"month_day\", how=\"inner\")\n",
        "final_merged_df = final_merged_df.drop(\"month_day\")\n",
        "final_merged_df = final_merged_df.drop(\"adjusted_price\")\n",
        "\n",
        "columns_to_drop = [col for col in final_merged_df.columns if col.endswith(\"price\") and col != \"price\"]\n",
        "final_merged_df = final_merged_df.drop(*columns_to_drop)\n",
        "\n",
        "final_merged_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZQgGhQd4j87",
        "outputId": "5b89c203-41d4-402b-fc91-89b3c732b1bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id',\n",
              " 'name',\n",
              " 'host_id',\n",
              " 'host_name',\n",
              " 'neighbourhood_group',\n",
              " 'neighbourhood',\n",
              " 'latitude',\n",
              " 'longitude',\n",
              " 'room_type',\n",
              " 'number_of_reviews',\n",
              " 'last_review',\n",
              " 'reviews_per_month',\n",
              " 'calculated_host_listings_count',\n",
              " 'availability_365',\n",
              " 'number_of_reviews_ltm',\n",
              " 'license',\n",
              " 'days_since_last_review',\n",
              " 'room_type_encoded',\n",
              " 'date',\n",
              " 'available',\n",
              " 'price',\n",
              " 'minimum_nights',\n",
              " 'maximum_nights',\n",
              " 'availability_percentage',\n",
              " 'Germany_Tourists',\n",
              " 'Spain_Tourists',\n",
              " 'France_Tourists',\n",
              " 'Italy_Tourists',\n",
              " 'UK_Tourists',\n",
              " 'Total_Tourists']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "final_merged_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0iNUxcX3RQG"
      },
      "source": [
        "## Data Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uurA0ISE4Yzb",
        "outputId": "a091f763-3925-47db-e1cc-64da81b9933d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|   id|                name|host_id|host_name|neighbourhood_group|       neighbourhood|latitude|longitude|      room_type|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|    license|days_since_last_review|room_type_encoded|      date|available|price|minimum_nights|maximum_nights|availability_percentage|Germany_Tourists|Spain_Tourists|France_Tourists|Italy_Tourists|UK_Tourists|Total_Tourists|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-03-21|        t|205.0|             2|           364|      78.63013698630137|              61|           247|             57|            73|         65|           503|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-05-27|        t|205.0|             2|           364|      78.63013698630137|              66|           227|             60|            92|         85|           530|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-03-29|        t|205.0|             2|           364|      78.63013698630137|              69|           272|             72|            89|         71|           573|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-05-02|        t|205.0|             2|           364|      78.63013698630137|              67|           259|             73|            99|         79|           577|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-01-14|        f|205.0|             2|           364|      78.63013698630137|              43|           214|             39|            66|         61|           423|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "filtered_df = final_merged_df.filter(col(\"price\") >= 0)\n",
        "filtered_df = filtered_df.filter((col(\"date\") >= \"2020-01-01\") & (col(\"date\") <= \"2024-06-15\"))\n",
        "filtered_df = filtered_df.dropna(subset=[\"price\", \"availability_percentage\"])\n",
        "filtered_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnhCPcCR-ugJ",
        "outputId": "49a08b97-0cfb-49c0-bf3d-36cc4564d5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows after filtering: 1852749\n",
            "Number of columns: 30\n"
          ]
        }
      ],
      "source": [
        "row_count = filtered_df.count()\n",
        "column_count = len(filtered_df.columns)\n",
        "\n",
        "print(f\"Number of rows after filtering: {row_count}\")\n",
        "print(f\"Number of columns: {column_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoK6r6663Vmj"
      },
      "source": [
        "## Data Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KDVhU5D_3M3",
        "outputId": "a7477153-db9f-43c3-871b-b4f68fffccef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 0\n",
            "++\n",
            "||\n",
            "++\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "||\n",
            "++\n",
            "only showing top 20 rows\n",
            "\n",
            "+---+----+-------+---------+-------------------+-------------+--------+---------+---------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-------+----------------------+-----------------+----+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "| id|name|host_id|host_name|neighbourhood_group|neighbourhood|latitude|longitude|room_type|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|license|days_since_last_review|room_type_encoded|date|available|price|minimum_nights|maximum_nights|availability_percentage|Germany_Tourists|Spain_Tourists|France_Tourists|Italy_Tourists|UK_Tourists|Total_Tourists|\n",
            "+---+----+-------+---------+-------------------+-------------+--------+---------+---------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-------+----------------------+-----------------+----+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|  0|   0|      0|        0|                  0|            0|       0|        0|        0|                0|          0|                0|                             0|               0|                    0|      0|                     0|                0|   0|        0|    0|             0|             0|                      0|               0|             0|              0|             0|          0|             0|\n",
            "+---+----+-------+---------+-------------------+-------------+--------+---------+---------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-------+----------------------+-----------------+----+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- host_id: integer (nullable = true)\n",
            " |-- host_name: string (nullable = true)\n",
            " |-- neighbourhood_group: string (nullable = true)\n",
            " |-- neighbourhood: string (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- number_of_reviews: integer (nullable = true)\n",
            " |-- last_review: date (nullable = true)\n",
            " |-- reviews_per_month: double (nullable = false)\n",
            " |-- calculated_host_listings_count: integer (nullable = true)\n",
            " |-- availability_365: integer (nullable = true)\n",
            " |-- number_of_reviews_ltm: integer (nullable = true)\n",
            " |-- license: string (nullable = false)\n",
            " |-- days_since_last_review: integer (nullable = true)\n",
            " |-- room_type_encoded: integer (nullable = false)\n",
            " |-- date: date (nullable = true)\n",
            " |-- available: string (nullable = true)\n",
            " |-- price: float (nullable = false)\n",
            " |-- minimum_nights: integer (nullable = true)\n",
            " |-- maximum_nights: integer (nullable = true)\n",
            " |-- availability_percentage: double (nullable = true)\n",
            " |-- Germany_Tourists: long (nullable = false)\n",
            " |-- Spain_Tourists: long (nullable = false)\n",
            " |-- France_Tourists: long (nullable = false)\n",
            " |-- Italy_Tourists: long (nullable = false)\n",
            " |-- UK_Tourists: long (nullable = false)\n",
            " |-- Total_Tourists: long (nullable = false)\n",
            "\n",
            "Number of invalid price values: 0\n",
            "Number of invalid availability_percentage values: 0\n",
            "Number of unique ids: 10058, Total ids: 1852749\n",
            "Number of invalid latitude values: 0\n",
            "Number of invalid longitude values: 0\n",
            "+-------+------------------+------------------+------------------+------------------+------------------+-----------------------+\n",
            "|summary|             price|    minimum_nights| number_of_reviews| reviews_per_month|  availability_365|availability_percentage|\n",
            "+-------+------------------+------------------+------------------+------------------+------------------+-----------------------+\n",
            "|  count|           1852749|           1852749|           1852749|           1852749|           1852749|                1852749|\n",
            "|   mean|282.94710130730067| 8.638110720880162| 72.18894194518523|1.8391745401017536| 186.6933275905155|      57.61262582176956|\n",
            "| stddev| 745.0160948162579|21.303773488300134|108.79597821542934|  2.05048114270532|132.35903917429405|      33.13317147688888|\n",
            "|    min|              10.0|                 1|                 1|              0.01|                 0|                    0.0|\n",
            "|    max|           15000.0|              1124|              1941|              67.3|               365|                  100.0|\n",
            "+-------+------------------+------------------+------------------+------------------+------------------+-----------------------+\n",
            "\n",
            "+---------------+-------+\n",
            "|      room_type|  count|\n",
            "+---------------+-------+\n",
            "|    Shared room|  19705|\n",
            "|     Hotel room|  19876|\n",
            "|Entire home/apt|1256383|\n",
            "|   Private room| 556785|\n",
            "+---------------+-------+\n",
            "\n",
            "+-------------------+------+\n",
            "|neighbourhood_group| count|\n",
            "+-------------------+------+\n",
            "|             Gràcia|160921|\n",
            "|         Sant Martí|169802|\n",
            "|     Horta-Guinardó| 46730|\n",
            "|          Les Corts| 36444|\n",
            "|     Sants-Montjuïc|200388|\n",
            "|         Nou Barris| 14980|\n",
            "|Sarrià-Sant Gervasi| 84876|\n",
            "|           Eixample|735338|\n",
            "|        Sant Andreu| 19605|\n",
            "|       Ciutat Vella|383665|\n",
            "+-------------------+------+\n",
            "\n",
            "Number of invalid last_review values: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicate rows\n",
        "duplicate_count = filtered_df.count() - filtered_df.dropDuplicates().count()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
        "\n",
        "# Separate numeric and non-numeric columns\n",
        "numeric_columns = [field.name for field in filtered_df.schema.fields if field.dataType in ['IntegerType', 'LongType', 'FloatType', 'DoubleType']]\n",
        "non_numeric_columns = [field.name for field in filtered_df.schema.fields if field.dataType not in ['IntegerType', 'LongType', 'FloatType', 'DoubleType']]\n",
        "\n",
        "# Check for null and missing values in numeric columns\n",
        "null_counts_numeric = filtered_df.select([count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in numeric_columns])\n",
        "null_counts_numeric.show()\n",
        "\n",
        "# Check for null values in non-numeric columns\n",
        "null_counts_non_numeric = filtered_df.select([count(when(col(c).isNull(), c)).alias(c) for c in non_numeric_columns])\n",
        "null_counts_non_numeric.show()\n",
        "\n",
        "# Verify data types\n",
        "filtered_df.printSchema()\n",
        "\n",
        "# Price should be non-negative\n",
        "invalid_prices = filtered_df.filter(col(\"price\") < 0).count()\n",
        "print(f\"Number of invalid price values: {invalid_prices}\")\n",
        "\n",
        "# Availability percentage should be between 0 and 100\n",
        "invalid_availability = filtered_df.filter((col(\"availability_percentage\") < 0) | (col(\"availability_percentage\") > 100)).count()\n",
        "print(f\"Number of invalid availability_percentage values: {invalid_availability}\")\n",
        "\n",
        "# Uniqueness check for IDs\n",
        "unique_id_count = filtered_df.select(\"id\").distinct().count()\n",
        "total_id_count = filtered_df.count()\n",
        "print(f\"Number of unique ids: {unique_id_count}, Total ids: {total_id_count}\")\n",
        "\n",
        "# Latitude should be between -90 and 90\n",
        "invalid_latitude = filtered_df.filter((col(\"latitude\") < -90) | (col(\"latitude\") > 90)).count()\n",
        "print(f\"Number of invalid latitude values: {invalid_latitude}\")\n",
        "\n",
        "# Longitude should be between -180 and 180\n",
        "invalid_longitude = filtered_df.filter((col(\"longitude\") < -180) | (col(\"longitude\") > 180)).count()\n",
        "print(f\"Number of invalid longitude values: {invalid_longitude}\")\n",
        "\n",
        "# Summary statistics\n",
        "filtered_df.describe([\"price\", \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\", \"availability_365\", \"availability_percentage\"]).show()\n",
        "\n",
        "# Distribution checks for specific columns\n",
        "filtered_df.groupBy(\"room_type\").count().show()\n",
        "filtered_df.groupBy(\"neighbourhood_group\").count().show()\n",
        "\n",
        "# Consistency checks : Last review should be before the current date\n",
        "invalid_last_review = filtered_df.filter(col(\"last_review\") > current_date()).count()\n",
        "print(f\"Number of invalid last_review values: {invalid_last_review}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW_rzGWwDNcv"
      },
      "source": [
        "## Store to Formatted Zone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "m8SqANKWK8BT"
      },
      "outputs": [],
      "source": [
        "delta_lake_path = \"/content/drive/MyDrive/BDM/Data/Formatted_Zone/filtered_df\"\n",
        "\n",
        "filtered_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").save(delta_lake_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUgmPfPOA3g2"
      },
      "source": [
        "# Exploitation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yOn-WmXA8aL"
      },
      "source": [
        "## Descriptive Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a03K0oA4ktsP",
        "outputId": "de5aea6d-f99a-47a6-84e3-6a19da309919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- host_id: integer (nullable = true)\n",
            " |-- host_name: string (nullable = true)\n",
            " |-- neighbourhood_group: string (nullable = true)\n",
            " |-- neighbourhood: string (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- number_of_reviews: integer (nullable = true)\n",
            " |-- last_review: date (nullable = true)\n",
            " |-- reviews_per_month: double (nullable = true)\n",
            " |-- calculated_host_listings_count: integer (nullable = true)\n",
            " |-- availability_365: integer (nullable = true)\n",
            " |-- number_of_reviews_ltm: integer (nullable = true)\n",
            " |-- license: string (nullable = true)\n",
            " |-- days_since_last_review: integer (nullable = true)\n",
            " |-- room_type_encoded: integer (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- available: string (nullable = true)\n",
            " |-- price: float (nullable = true)\n",
            " |-- minimum_nights: integer (nullable = true)\n",
            " |-- maximum_nights: integer (nullable = true)\n",
            " |-- availability_percentage: double (nullable = true)\n",
            " |-- Germany_Tourists: long (nullable = true)\n",
            " |-- Spain_Tourists: long (nullable = true)\n",
            " |-- France_Tourists: long (nullable = true)\n",
            " |-- Italy_Tourists: long (nullable = true)\n",
            " |-- UK_Tourists: long (nullable = true)\n",
            " |-- Total_Tourists: long (nullable = true)\n",
            "\n",
            "+-----+--------------------+-------+-------------+-------------------+------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|   id|                name|host_id|    host_name|neighbourhood_group|     neighbourhood|latitude|longitude|      room_type|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|    license|days_since_last_review|room_type_encoded|      date|available|price|minimum_nights|maximum_nights|availability_percentage|Germany_Tourists|Spain_Tourists|France_Tourists|Italy_Tourists|UK_Tourists|Total_Tourists|\n",
            "+-----+--------------------+-------+-------------+-------------------+------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-06-14|        f| 95.0|             4|          1125|     10.684931506849315|              72|           249|             62|           100|         82|           565|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-03-14|        f| 95.0|             3|          1125|     10.684931506849315|              60|           238|             60|            81|         66|           505|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-05-01|        f| 95.0|             4|          1125|     10.684931506849315|              69|           270|             57|           101|         78|           575|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-03-26|        f| 95.0|             4|          1125|     10.684931506849315|              50|           231|             63|            93|         85|           522|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-06-15|        f| 95.0|             4|          1125|     10.684931506849315|              71|           289|             80|           105|         92|           637|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-04-22|        f| 95.0|             4|          1125|     10.684931506849315|              63|           233|             56|            91|         82|           525|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-03-17|        f| 95.0|             3|          1125|     10.684931506849315|              48|           244|             77|            78|         82|           529|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-06-04|        f| 95.0|             4|          1125|     10.684931506849315|              65|           254|             77|            87|         97|           580|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-04-02|        f| 95.0|             4|          1125|     10.684931506849315|              73|           255|             77|            82|         86|           573|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-10|        f| 95.0|             2|          1125|     10.684931506849315|              59|           252|             63|            87|         78|           539|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-16|        f| 95.0|             2|          1125|     10.684931506849315|              60|           226|             55|            76|         77|           494|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-23|        f| 95.0|             3|          1125|     10.684931506849315|              57|           227|             58|            78|         70|           490|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-01-25|        f| 95.0|             2|          1125|     10.684931506849315|              38|           224|             47|            55|         52|           416|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-01-21|        t| 95.0|             2|          1125|     10.684931506849315|              41|           212|             43|            66|         58|           420|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-28|        f| 95.0|             4|          1125|     10.684931506849315|              61|           231|             55|            71|         64|           482|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-26|        f| 95.0|             4|          1125|     10.684931506849315|              55|           242|             65|            79|         77|           518|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-05-22|        f| 95.0|             4|          1125|     10.684931506849315|              75|           277|             89|           108|         80|           629|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-03-29|        f| 95.0|             4|          1125|     10.684931506849315|              69|           272|             72|            89|         71|           573|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-01-30|        t| 95.0|             3|          1125|     10.684931506849315|              54|           200|             50|            69|         57|           430|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-25|        f| 95.0|             4|          1125|     10.684931506849315|              50|           207|             52|            73|         66|           448|\n",
            "+-----+--------------------+-------+-------------+-------------------+------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "delta_lake_path = \"/content/drive/MyDrive/BDM/Data/Formatted_Zone/filtered_df\"\n",
        "\n",
        "# Read the Delta Lake table\n",
        "filtered_df = spark.read.format(\"delta\").load(delta_lake_path)\n",
        "filtered_df.printSchema()\n",
        "filtered_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlLO14scITaY"
      },
      "source": [
        "### KPIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akW8-2HXBBMK",
        "outputId": "12f88058-976d-4183-f086-6c9425d9ac5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------------+\n",
            "|neighbourhood_group|     average_price|\n",
            "+-------------------+------------------+\n",
            "|             Gràcia|252.91087552277205|\n",
            "|         Sant Martí| 309.8315567543374|\n",
            "|     Horta-Guinardó|129.51123475283543|\n",
            "|          Les Corts|172.57770826473492|\n",
            "|     Sants-Montjuïc|201.66637722817734|\n",
            "|         Nou Barris| 95.39879839786381|\n",
            "|Sarrià-Sant Gervasi|206.81039398652152|\n",
            "|           Eixample| 361.8364371214326|\n",
            "|        Sant Andreu|179.96796735526652|\n",
            "|       Ciutat Vella|233.49953475036816|\n",
            "+-------------------+------------------+\n",
            "\n",
            "+---------------+------------------+\n",
            "|      room_type|     average_price|\n",
            "+---------------+------------------+\n",
            "|    Shared room| 218.7249429078914|\n",
            "|     Hotel room|1315.9984906419802|\n",
            "|Entire home/apt|313.38256805448657|\n",
            "|   Private room|179.66480239230583|\n",
            "+---------------+------------------+\n",
            "\n",
            "+-------------------+------------------+\n",
            "|neighbourhood_group| availability_rate|\n",
            "+-------------------+------------------+\n",
            "|             Gràcia|60.285891175390205|\n",
            "|         Sant Martí| 58.84857948789674|\n",
            "|     Horta-Guinardó| 59.28015664858384|\n",
            "|          Les Corts| 52.46550739677819|\n",
            "|     Sants-Montjuïc|57.030242944080804|\n",
            "|         Nou Barris|56.391029844317885|\n",
            "|Sarrià-Sant Gervasi| 65.88550901789945|\n",
            "|           Eixample| 59.41878639723153|\n",
            "|        Sant Andreu| 61.05929127745021|\n",
            "|       Ciutat Vella|  51.1140616112743|\n",
            "+-------------------+------------------+\n",
            "\n",
            "+-----+--------------+\n",
            "|month|total_tourists|\n",
            "+-----+--------------+\n",
            "|   12|      99339025|\n",
            "|    1|     135430970|\n",
            "|    6|      87645412|\n",
            "|    3|     158916400|\n",
            "|    5|     186565842|\n",
            "|    4|     175934536|\n",
            "|    2|     137512976|\n",
            "+-----+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Average price per area\n",
        "avg_price_per_neighbourhood_group = filtered_df.groupBy(\"neighbourhood_group\").agg(F.avg(\"price\").alias(\"average_price\"))\n",
        "avg_price_per_neighbourhood_group.show()\n",
        "\n",
        "# Average price per room type\n",
        "avg_price_per_room_type = filtered_df.groupBy(\"room_type\").agg(F.avg(\"price\").alias(\"average_price\"))\n",
        "avg_price_per_room_type.show()\n",
        "\n",
        "# Vacancy rate per area\n",
        "availability_rate_per_neighbourhood_group = filtered_df.groupBy(\"neighbourhood_group\").agg(F.avg(\"availability_percentage\").alias(\"availability_rate\"))\n",
        "availability_rate_per_neighbourhood_group.show()\n",
        "\n",
        "# Trends in the total number of tourists per month\n",
        "monthly_tourists_trend = filtered_df.groupBy(F.month(\"date\").alias(\"month\")).agg(F.sum(\"Total_Tourists\").alias(\"total_tourists\"))\n",
        "monthly_tourists_trend.show()\n",
        "\n",
        "# Save KPIs\n",
        "avg_price_per_neighbourhood_group.coalesce(1).write.csv(\"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/KPIs/avg_price_per_neighbourhood_group.csv\", header=True, mode=\"overwrite\")\n",
        "avg_price_per_room_type.coalesce(1).write.csv(\"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/KPIs/avg_price_per_room_type.csv\", header=True, mode=\"overwrite\")\n",
        "availability_rate_per_neighbourhood_group.coalesce(1).write.csv(\"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/KPIs/availability_rate_per_neighbourhood_group.csv\", header=True, mode=\"overwrite\")\n",
        "monthly_tourists_trend.coalesce(1).write.csv(\"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/KPIs/monthly_tourists_trend.csv\", header=True, mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huwSV91OIVV1"
      },
      "source": [
        "## Predictive Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delta_lake_path = \"/content/drive/MyDrive/BDM/Data/Formatted_Zone/filtered_df\"\n",
        "\n",
        "# Read the Delta Lake table\n",
        "filtered_df = spark.read.format(\"delta\").load(delta_lake_path)\n",
        "filtered_df.printSchema()\n",
        "filtered_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beEnfDt9m1pk",
        "outputId": "75299470-4a96-42b6-8ec7-62514427741e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- host_id: integer (nullable = true)\n",
            " |-- host_name: string (nullable = true)\n",
            " |-- neighbourhood_group: string (nullable = true)\n",
            " |-- neighbourhood: string (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- number_of_reviews: integer (nullable = true)\n",
            " |-- last_review: date (nullable = true)\n",
            " |-- reviews_per_month: double (nullable = true)\n",
            " |-- calculated_host_listings_count: integer (nullable = true)\n",
            " |-- availability_365: integer (nullable = true)\n",
            " |-- number_of_reviews_ltm: integer (nullable = true)\n",
            " |-- license: string (nullable = true)\n",
            " |-- days_since_last_review: integer (nullable = true)\n",
            " |-- room_type_encoded: integer (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- available: string (nullable = true)\n",
            " |-- price: float (nullable = true)\n",
            " |-- minimum_nights: integer (nullable = true)\n",
            " |-- maximum_nights: integer (nullable = true)\n",
            " |-- availability_percentage: double (nullable = true)\n",
            " |-- Germany_Tourists: long (nullable = true)\n",
            " |-- Spain_Tourists: long (nullable = true)\n",
            " |-- France_Tourists: long (nullable = true)\n",
            " |-- Italy_Tourists: long (nullable = true)\n",
            " |-- UK_Tourists: long (nullable = true)\n",
            " |-- Total_Tourists: long (nullable = true)\n",
            "\n",
            "+-----+--------------------+-------+-------------+-------------------+------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|   id|                name|host_id|    host_name|neighbourhood_group|     neighbourhood|latitude|longitude|      room_type|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|    license|days_since_last_review|room_type_encoded|      date|available|price|minimum_nights|maximum_nights|availability_percentage|Germany_Tourists|Spain_Tourists|France_Tourists|Italy_Tourists|UK_Tourists|Total_Tourists|\n",
            "+-----+--------------------+-------+-------------+-------------------+------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-06-14|        f| 95.0|             4|          1125|     10.684931506849315|              72|           249|             62|           100|         82|           565|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-03-14|        f| 95.0|             3|          1125|     10.684931506849315|              60|           238|             60|            81|         66|           505|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-05-01|        f| 95.0|             4|          1125|     10.684931506849315|              69|           270|             57|           101|         78|           575|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-03-26|        f| 95.0|             4|          1125|     10.684931506849315|              50|           231|             63|            93|         85|           522|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-06-15|        f| 95.0|             4|          1125|     10.684931506849315|              71|           289|             80|           105|         92|           637|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-04-22|        f| 95.0|             4|          1125|     10.684931506849315|              63|           233|             56|            91|         82|           525|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-03-17|        f| 95.0|             3|          1125|     10.684931506849315|              48|           244|             77|            78|         82|           529|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-06-04|        f| 95.0|             4|          1125|     10.684931506849315|              65|           254|             77|            87|         97|           580|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-04-02|        f| 95.0|             4|          1125|     10.684931506849315|              73|           255|             77|            82|         86|           573|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-10|        f| 95.0|             2|          1125|     10.684931506849315|              59|           252|             63|            87|         78|           539|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-16|        f| 95.0|             2|          1125|     10.684931506849315|              60|           226|             55|            76|         77|           494|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-23|        f| 95.0|             3|          1125|     10.684931506849315|              57|           227|             58|            78|         70|           490|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-01-25|        f| 95.0|             2|          1125|     10.684931506849315|              38|           224|             47|            55|         52|           416|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-01-21|        t| 95.0|             2|          1125|     10.684931506849315|              41|           212|             43|            66|         58|           420|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-28|        f| 95.0|             4|          1125|     10.684931506849315|              61|           231|             55|            71|         64|           482|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-26|        f| 95.0|             4|          1125|     10.684931506849315|              55|           242|             65|            79|         77|           518|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-05-22|        f| 95.0|             4|          1125|     10.684931506849315|              75|           277|             89|           108|         80|           629|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-03-29|        f| 95.0|             4|          1125|     10.684931506849315|              69|           272|             72|            89|         71|           573|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-01-30|        t| 95.0|             3|          1125|     10.684931506849315|              54|           200|             50|            69|         57|           430|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-02-25|        f| 95.0|             4|          1125|     10.684931506849315|              50|           207|             52|            73|         66|           448|\n",
            "+-----+--------------------+-------+-------------+-------------------+------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, mean\n",
        "\n",
        "mean_total_tourists = filtered_df.select(mean(col(\"Total_Tourists\"))).collect()[0][0]\n",
        "\n",
        "filtered_df = filtered_df.withColumn(\"Label\", (col(\"Total_Tourists\") > mean_total_tourists).cast(\"int\"))\n",
        "\n",
        "rdd_df = filtered_df.select(\"Germany_Tourists\", \"Italy_Tourists\", \"Label\").limit(100000)\n",
        "\n",
        "rdd = rdd_df.rdd\n",
        "\n",
        "print(rdd.take(5))\n",
        "\n",
        "labeled_data = rdd.map(lambda row: LabeledPoint(row.Label, [row.Germany_Tourists, row.Italy_Tourists]))\n",
        "\n",
        "train_data, test_data = labeled_data.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4-dtEokT6sr",
        "outputId": "5a362ff4-feb5-4188-e541-6eac868e1838"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(Germany_Tourists=57, Italy_Tourists=101, Label=1), Row(Germany_Tourists=62, Italy_Tourists=109, Label=1), Row(Germany_Tourists=52, Italy_Tourists=92, Label=0), Row(Germany_Tourists=57, Italy_Tourists=103, Label=1), Row(Germany_Tourists=64, Italy_Tourists=95, Label=1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegressionWithSGD.train(train_data, iterations=10)\n",
        "\n",
        "test_features = test_data.map(lambda x: x.features)\n",
        "predictions = model.predict(test_features)\n",
        "\n",
        "labels_and_predictions = test_data.map(lambda lp: lp.label).zip(predictions)\n",
        "\n",
        "print(\"Labels and Predictions: \", labels_and_predictions.take(10))\n",
        "\n",
        "accuracy = labels_and_predictions.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJljEla8C2T5",
        "outputId": "0daf0036-8765-4772-9b6a-a3719a321cc6"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels and Predictions:  [(1.0, 0), (0.0, 0), (1.0, 0), (0.0, 0), (0.0, 0), (0.0, 0), (0.0, 0), (0.0, 0), (0.0, 0), (1.0, 0)]\n",
            "Model accuracy: 51.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将预测结果转换为DataFrame\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType\n",
        "schema = StructType([StructField(\"label\", DoubleType(), True), StructField(\"prediction\", DoubleType(), True)])\n",
        "predictions_df = spark.createDataFrame(valid_predictions, schema)\n",
        "\n",
        "# 计算均方误差\n",
        "from pyspark.sql.functions import col\n",
        "mse = predictions_df.withColumn(\"squared_error\", (col(\"label\") - col(\"prediction\")) ** 2).agg({\"squared_error\": \"mean\"}).collect()[0][\"avg(squared_error)\"]\n",
        "print(f\"Mean Squared Error = {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Oy0OULxH2ZJG",
        "outputId": "bf28becb-3a97-4176-a5a5-b4c3f3523cbd"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o2644.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2045.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2045.0 (TID 13192) (6c6d0de9794f executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `-1.7801352675494917e+126` in type `float64`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `-1.7801352675494917e+126` in type `float64`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-a674faf4a1d4>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 计算均方误差\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"squared_error\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg(squared_error)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mean Squared Error = {mse}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \"\"\"\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2644.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2045.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2045.0 (TID 13192) (6c6d0de9794f executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `-1.7801352675494917e+126` in type `float64`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `-1.7801352675494917e+126` in type `float64`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxWSAc2gIcH7"
      },
      "source": [
        "### Data Splict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTI1nKL_XCZC",
        "outputId": "edcd009d-fc55-4cc8-9e48-e949bab345ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- host_id: integer (nullable = true)\n",
            " |-- host_name: string (nullable = true)\n",
            " |-- neighbourhood_group: string (nullable = true)\n",
            " |-- neighbourhood: string (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- number_of_reviews: integer (nullable = true)\n",
            " |-- last_review: date (nullable = true)\n",
            " |-- reviews_per_month: double (nullable = true)\n",
            " |-- calculated_host_listings_count: integer (nullable = true)\n",
            " |-- availability_365: integer (nullable = true)\n",
            " |-- number_of_reviews_ltm: integer (nullable = true)\n",
            " |-- license: string (nullable = true)\n",
            " |-- days_since_last_review: integer (nullable = true)\n",
            " |-- room_type_encoded: integer (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- available: string (nullable = true)\n",
            " |-- price: float (nullable = true)\n",
            " |-- minimum_nights: integer (nullable = true)\n",
            " |-- maximum_nights: integer (nullable = true)\n",
            " |-- availability_percentage: double (nullable = true)\n",
            " |-- Germany_Tourists: long (nullable = true)\n",
            " |-- Spain_Tourists: long (nullable = true)\n",
            " |-- France_Tourists: long (nullable = true)\n",
            " |-- Italy_Tourists: long (nullable = true)\n",
            " |-- UK_Tourists: long (nullable = true)\n",
            " |-- Total_Tourists: long (nullable = true)\n",
            "\n",
            "+-----+--------------------+-------+-------------+-------------------+------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|   id|                name|host_id|    host_name|neighbourhood_group|     neighbourhood|latitude|longitude|      room_type|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|    license|days_since_last_review|room_type_encoded|      date|available|price|minimum_nights|maximum_nights|availability_percentage|Germany_Tourists|Spain_Tourists|France_Tourists|Italy_Tourists|UK_Tourists|Total_Tourists|\n",
            "+-----+--------------------+-------+-------------+-------------------+------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-06-14|        f| 95.0|             4|          1125|     10.684931506849315|              72|           249|             62|           100|         82|           565|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-03-14|        f| 95.0|             3|          1125|     10.684931506849315|              60|           238|             60|            81|         66|           505|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-05-01|        f| 95.0|             4|          1125|     10.684931506849315|              69|           270|             57|           101|         78|           575|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-03-26|        f| 95.0|             4|          1125|     10.684931506849315|              50|           231|             63|            93|         85|           522|\n",
            "|18674|Rental unit in Ba...|  71615|Mireia  Maria|           Eixample|la Sagrada Família|41.40556|  2.17262|Entire home/apt|               40| 2023-11-07|             0.31|                            30|              39|                    7|HUTB-002062|                   225|                1|2024-06-15|        f| 95.0|             4|          1125|     10.684931506849315|              71|           289|             80|           105|         92|           637|\n",
            "+-----+--------------------+-------+-------------+-------------------+------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+-----------------+\n",
            "|   id|                name|host_id|host_name|neighbourhood_group|       neighbourhood|latitude|longitude|      room_type|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|    license|days_since_last_review|room_type_encoded|      date|available|price|minimum_nights|maximum_nights|availability_percentage|Germany_Tourists|Spain_Tourists|France_Tourists|Italy_Tourists|UK_Tourists|Total_Tourists|next_day_tourists|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+-----------------+\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-14|        f|205.0|             4|           364|      78.63013698630137|              57|           249|             73|           101|         76|           556|              592|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-15|        f|205.0|             4|           364|      78.63013698630137|              62|           261|             84|           109|         76|           592|              489|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-16|        f|205.0|             4|           364|      78.63013698630137|              52|           227|             46|            92|         72|           489|              544|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-17|        f|205.0|             4|           364|      78.63013698630137|              57|           233|             70|           103|         81|           544|              579|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-18|        f|205.0|             1|           364|      78.63013698630137|              64|           262|             74|            95|         84|           579|              544|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+-----------------+\n",
            "|   id|                name|host_id|host_name|neighbourhood_group|       neighbourhood|latitude|longitude|      room_type|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|    license|days_since_last_review|room_type_encoded|      date|available|price|minimum_nights|maximum_nights|availability_percentage|Germany_Tourists|Spain_Tourists|France_Tourists|Italy_Tourists|UK_Tourists|Total_Tourists|next_day_tourists|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+-----------------+\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-14|        f|205.0|             4|           364|      78.63013698630137|              57|           249|             73|           101|         76|           556|              592|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-15|        f|205.0|             4|           364|      78.63013698630137|              62|           261|             84|           109|         76|           592|              489|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-17|        f|205.0|             4|           364|      78.63013698630137|              57|           233|             70|           103|         81|           544|              579|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-18|        f|205.0|             1|           364|      78.63013698630137|              64|           262|             74|            95|         84|           579|              544|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-19|        f|205.0|             2|           364|      78.63013698630137|              61|           265|             61|            90|         67|           544|              571|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+-----------------+\n",
            "|   id|                name|host_id|host_name|neighbourhood_group|       neighbourhood|latitude|longitude|      room_type|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|    license|days_since_last_review|room_type_encoded|      date|available|price|minimum_nights|maximum_nights|availability_percentage|Germany_Tourists|Spain_Tourists|France_Tourists|Italy_Tourists|UK_Tourists|Total_Tourists|next_day_tourists|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+-----------------+\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-16|        f|205.0|             4|           364|      78.63013698630137|              52|           227|             46|            92|         72|           489|              544|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-20|        f|205.0|             2|           364|      78.63013698630137|              56|           258|             79|            98|         80|           571|              579|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-22|        f|205.0|             2|           364|      78.63013698630137|              65|           269|             89|           113|         83|           619|              539|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2023-12-27|        f|205.0|             2|           364|      78.63013698630137|              54|           247|             81|            95|         81|           558|              562|\n",
            "|40983|Condo in Barcelon...| 177617|  Joaquin|           Eixample|la Dreta de l'Eix...|41.39631|  2.16832|Entire home/apt|              290| 2023-11-16|             1.91|                             6|             287|                   39|HUTB-001282|                   216|                1|2024-01-02|        t|205.0|             1|           364|      78.63013698630137|              56|           212|             67|            82|         73|           490|              477|\n",
            "+-----+--------------------+-------+---------+-------------------+--------------------+--------+---------+---------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+----------------------+-----------------+----------+---------+-----+--------------+--------------+-----------------------+----------------+--------------+---------------+--------------+-----------+--------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "filtered_df = filtered_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "filtered_df.printSchema()\n",
        "filtered_df.show(5)\n",
        "\n",
        "window_spec = Window.partitionBy(\"id\").orderBy(\"date\")\n",
        "\n",
        "filtered_df = filtered_df.withColumn(\"next_day_tourists\", lead(\"Total_Tourists\").over(window_spec))\n",
        "\n",
        "filtered_df = filtered_df.filter(col(\"next_day_tourists\").isNotNull())\n",
        "\n",
        "filtered_df.show(5)\n",
        "\n",
        "feature_columns = [\n",
        "    \"host_id\", \"latitude\", \"longitude\", \"room_type_encoded\", \"number_of_reviews\",\n",
        "    \"reviews_per_month\", \"calculated_host_listings_count\", \"availability_365\",\n",
        "    \"days_since_last_review\", \"price\", \"minimum_nights\", \"maximum_nights\",\n",
        "    \"availability_percentage\", \"Germany_Tourists\", \"Spain_Tourists\", \"France_Tourists\",\n",
        "    \"Italy_Tourists\", \"UK_Tourists\", \"Total_Tourists\"\n",
        "]\n",
        "target_column = \"next_day_tourists\"\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_df, test_df = filtered_df.randomSplit([train_ratio, 1 - train_ratio], seed=42)\n",
        "\n",
        "train_df.show(5)\n",
        "test_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1eD8Il_IeHf"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1oVs-80IX61",
        "outputId": "3e83cc67-d515-4571-9651-7cc5aa7117cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+-----------------+\n",
            "|            features|label|       prediction|\n",
            "+--------------------+-----+-----------------+\n",
            "|[177617.0,41.3963...|  544|527.7573673722007|\n",
            "|[177617.0,41.3963...|  579|533.7668767812655|\n",
            "|[177617.0,41.3963...|  539|565.6336362187195|\n",
            "|[177617.0,41.3963...|  562|523.4028226708232|\n",
            "|[177617.0,41.3963...|  477| 502.604398591066|\n",
            "+--------------------+-----+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the features and label columns for the VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Transform the training and test sets\n",
        "train_df = assembler.transform(train_df).select(\"features\", col(target_column).alias(\"label\"))\n",
        "test_df = assembler.transform(test_df).select(\"features\", col(target_column).alias(\"label\"))\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "lr_model = lr.fit(train_df)\n",
        "\n",
        "predictions = lr_model.transform(test_df)\n",
        "\n",
        "predictions.select(\"features\", \"label\", \"prediction\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENZCXmLhVQWW"
      },
      "source": [
        "### Model Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai1jj3Y18nGe",
        "outputId": "05b72a88-0589-4935-d748-d2ba01b8aacf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) on test data = 48.87104892571636\n",
            "Mean Absolute Error (MAE) on test data = 40.433859916921726\n",
            "R-squared (R2) on test data = 0.5263993754115708\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model using RMSE, MAE, and R2\n",
        "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator_rmse.evaluate(predictions)\n",
        "\n",
        "evaluator_mae = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "mae = evaluator_mae.evaluate(predictions)\n",
        "\n",
        "evaluator_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2 = evaluator_r2.evaluate(predictions)\n",
        "\n",
        "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
        "print(f\"Mean Absolute Error (MAE) on test data = {mae}\")\n",
        "print(f\"R-squared (R2) on test data = {r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_utg7VjVSbF"
      },
      "source": [
        "### Model & Predictions Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "2HSRfI-MWMft",
        "outputId": "10d4120c-b4b0-44e3-b643-cb601d1252d3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `label`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             return _infer_schema(\n\u001b[0m\u001b[1;32m   1631\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1669\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m   1671\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CANNOT_INFER_SCHEMA_FOR_TYPE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `float64`.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1680\u001b[0m                     \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m                     _infer_type(\n\u001b[0m\u001b[1;32m   1682\u001b[0m                         \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1635\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m             raise PySparkTypeError(\n\u001b[0m\u001b[1;32m   1637\u001b[0m                 \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"UNSUPPORTED_DATA_TYPE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [UNSUPPORTED_DATA_TYPE] Unsupported DataType `float64`.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-366a06d725c9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictions_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/predictions.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Predictions saved to {predictions_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0mtupled_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             schema = _infer_schema(\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1689\u001b[0m             )\n\u001b[1;32m   1690\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m             raise PySparkTypeError(\n\u001b[0m\u001b[1;32m   1692\u001b[0m                 \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CANNOT_INFER_TYPE_FOR_FIELD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"field_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `label`."
          ]
        }
      ],
      "source": [
        "predictions_df = predictions.toDF([\"prediction\", \"label\"])\n",
        "\n",
        "predictions_path = \"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/predictions.csv\"\n",
        "predictions_df.write.mode(\"overwrite\").csv(predictions_path, header=True)\n",
        "print(f\"Predictions saved to {predictions_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RizSRfJp4i-u",
        "outputId": "bfb908b9-4d62-44c3-8b80-7bb73330c77c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o1725.save.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/content/drive/MyDrive/BDM/Data/Exploitation_Zone/lr_model/metadata already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n\tat org.apache.spark.mllib.regression.impl.GLMRegressionModel$SaveLoadV1_0$.save(GLMRegressionModel.scala:56)\n\tat org.apache.spark.mllib.regression.LinearRegressionModel.save(LinearRegression.scala:51)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-66f2af561e50>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/lr_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model saved to {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/mllib/regression.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sc, path)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         )\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mjava_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1725.save.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/content/drive/MyDrive/BDM/Data/Exploitation_Zone/lr_model/metadata already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n\tat org.apache.spark.mllib.regression.impl.GLMRegressionModel$SaveLoadV1_0$.save(GLMRegressionModel.scala:56)\n\tat org.apache.spark.mllib.regression.LinearRegressionModel.save(LinearRegression.scala:51)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ],
      "source": [
        "model_path = \"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/lr_model\"\n",
        "model.write().overwrite().save(model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pULKgmnnIne-"
      },
      "source": [
        "# Stream analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkmLPHLLdk5g"
      },
      "source": [
        "## Data Flow Simulator Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tGOm5npSIodm"
      },
      "outputs": [],
      "source": [
        "sample_df = filtered_df.limit(10).toPandas()\n",
        "\n",
        "sample_df['date'] = sample_df['date'].astype(str)\n",
        "sample_df['last_review'] = sample_df['last_review'].astype(str)\n",
        "\n",
        "data_dir = \"/content/data_stream\"\n",
        "os.makedirs(data_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcqTxlHidnBx"
      },
      "source": [
        "## Spark Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKK5UyCxebgq",
        "outputId": "8a52ef28-b5e4-4c61-cb8d-9676439e8c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            features  prediction  \\\n",
            "0  {'type': 1, 'values': [177617.0, 41.39631, 2.1...  527.757367   \n",
            "1  {'type': 1, 'values': [177617.0, 41.39631, 2.1...  559.923428   \n",
            "2  {'type': 1, 'values': [177617.0, 41.39631, 2.1...  531.275637   \n",
            "3  {'type': 1, 'values': [177617.0, 41.39631, 2.1...  533.766877   \n",
            "4  {'type': 1, 'values': [177617.0, 41.39631, 2.1...  538.804282   \n",
            "\n",
            "   Total_Tourists        date  \n",
            "0             489  2023-12-16  \n",
            "1             579  2023-12-18  \n",
            "2             544  2023-12-19  \n",
            "3             571  2023-12-20  \n",
            "4             539  2023-12-23  \n"
          ]
        }
      ],
      "source": [
        "for index, row in sample_df.iterrows():\n",
        "    message = row.to_dict()\n",
        "    file_path = os.path.join(data_dir, f\"data_{index}.json\")\n",
        "    with open(file_path, \"w\") as file:\n",
        "        json.dump(message, file)\n",
        "    time.sleep(1)\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", LongType(), True),\n",
        "    StructField(\"host_id\", IntegerType(), True),\n",
        "    StructField(\"latitude\", DoubleType(), True),\n",
        "    StructField(\"longitude\", DoubleType(), True),\n",
        "    StructField(\"number_of_reviews\", IntegerType(), True),\n",
        "    StructField(\"last_review\", StringType(), True),\n",
        "    StructField(\"reviews_per_month\", DoubleType(), True),\n",
        "    StructField(\"calculated_host_listings_count\", IntegerType(), True),\n",
        "    StructField(\"availability_365\", IntegerType(), True),\n",
        "    StructField(\"number_of_reviews_ltm\", IntegerType(), True),\n",
        "    StructField(\"days_since_last_review\", IntegerType(), True),\n",
        "    StructField(\"room_type_encoded\", IntegerType(), True),\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"price\", DoubleType(), True),\n",
        "    StructField(\"minimum_nights\", IntegerType(), True),\n",
        "    StructField(\"maximum_nights\", IntegerType(), True),\n",
        "    StructField(\"availability_percentage\", DoubleType(), True),\n",
        "    StructField(\"Germany_Tourists\", LongType(), True),\n",
        "    StructField(\"Spain_Tourists\", LongType(), True),\n",
        "    StructField(\"France_Tourists\", LongType(), True),\n",
        "    StructField(\"Italy_Tourists\", LongType(), True),\n",
        "    StructField(\"UK_Tourists\", LongType(), True),\n",
        "    StructField(\"Total_Tourists\", LongType(), True),\n",
        "])\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/lr_model\"\n",
        "lr_model = LinearRegressionModel.load(model_path)\n",
        "\n",
        "df_stream = spark \\\n",
        "    .readStream \\\n",
        "    .schema(schema) \\\n",
        "    .json(\"/content/data_stream\")\n",
        "\n",
        "feature_columns = [\n",
        "    \"host_id\", \"latitude\", \"longitude\", \"room_type_encoded\", \"number_of_reviews\",\n",
        "    \"reviews_per_month\", \"calculated_host_listings_count\", \"availability_365\",\n",
        "    \"days_since_last_review\", \"price\", \"minimum_nights\", \"maximum_nights\",\n",
        "    \"availability_percentage\", \"Germany_Tourists\", \"Spain_Tourists\", \"France_Tourists\",\n",
        "    \"Italy_Tourists\", \"UK_Tourists\", \"Total_Tourists\"\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "df_features = assembler.transform(df_stream)\n",
        "\n",
        "predictions = lr_model.transform(df_features)\n",
        "\n",
        "output_path = \"/content/predictions_output\"\n",
        "\n",
        "query = predictions.select(\"features\", \"prediction\", \"Total_Tourists\", \"date\").writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"json\") \\\n",
        "    .option(\"path\", output_path) \\\n",
        "    .option(\"checkpointLocation\", \"/content/checkpoint\") \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(20)\n",
        "query.stop()\n",
        "\n",
        "result_files = glob.glob(f\"{output_path}/*.json\")\n",
        "results = []\n",
        "\n",
        "for file in result_files:\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            results.append(json.loads(line))\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}